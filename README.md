# ETL Processor - Transformador de CSV para SQL

Uma aplicaÃ§Ã£o completa para transformar arquivos CSV em scripts SQL, com interface web intuitiva e API backend robusta.

## ğŸš€ Arquitetura

O projeto foi reestruturado para uma arquitetura moderna de microserviÃ§os:

- **Frontend**: Next.js 14 com TypeScript e Tailwind CSS
- **Backend**: Flask API em Python para processamento de dados
- **ContainerizaÃ§Ã£o**: Docker e Docker Compose para fÃ¡cil deployment

## âœ¨ Funcionalidades

### Processamento de Dados
- ğŸ“ Upload de arquivos CSV via drag & drop
- âš™ï¸ ConfiguraÃ§Ã£o flexÃ­vel de delimitadores (vÃ­rgula, ponto e vÃ­rgula, tab, pipe)
- ğŸ¯ SeleÃ§Ã£o e reordenaÃ§Ã£o de campos
- ğŸ¨ FormataÃ§Ã£o de dados (texto, nÃºmero, moeda, data)
- ğŸ‘€ PrÃ©-visualizaÃ§Ã£o dos dados processados
- ğŸ—„ï¸ GeraÃ§Ã£o de SQL para mÃºltiplos SGBDs (MySQL, PostgreSQL, SQLite, SQL Server, ANSI)
- ğŸ“¥ Download de arquivos processados (CSV, TSV, TXT, SQL)

### Modelagem de Data Warehouse â­ NOVO
- ğŸ—ï¸ AnÃ¡lise automÃ¡tica de esquemas SQL
- â­ GeraÃ§Ã£o de modelos dimensionais (Star Schema)
- ğŸ“Š IdentificaÃ§Ã£o automÃ¡tica de tabelas fato e dimensÃ£o
- ğŸ”„ Suporte a Slowly Changing Dimensions (SCD)
- ğŸ¯ RecomendaÃ§Ãµes de otimizaÃ§Ã£o
- ğŸ“‹ GeraÃ§Ã£o de DDL para mÃºltiplos SGBDs
- ğŸ” VisualizaÃ§Ã£o interativa do modelo dimensional
- ğŸ› ï¸ **Suporte aprimorado para dados de CSV**: Funciona com tabelas Ãºnicas geradas a partir de CSV
- ğŸ¤– **ClassificaÃ§Ã£o IA**: IntegraÃ§Ã£o com OpenRouter para classificaÃ§Ã£o inteligente de dimensÃµes

## ğŸ› ï¸ Tecnologias Utilizadas

### Frontend
- Next.js 14
- TypeScript
- Tailwind CSS
- Radix UI Components
- React Hook Form
- Lucide Icons

### Backend
- Flask 3.0
- Flask-CORS
- Python 3.11
- Gunicorn
- SQLParse (anÃ¡lise de SQL)
- Pandas & NumPy (processamento de dados)
- Engine de Modelagem Dimensional personalizado

### DevOps
- Docker
- Docker Compose
- Multi-stage builds

## ğŸš€ ExecuÃ§Ã£o com Docker Compose (Recomendado)

### PrÃ©-requisitos
- Docker
- Docker Compose

### Passos

1. **Clone o repositÃ³rio**
```bash
git clone <repository-url>
cd etl-processor
```

2. **Execute com Docker Compose**
```bash
docker-compose up --build
```

3. **Acesse a aplicaÃ§Ã£o**
- Frontend: http://localhost:3000
- Backend API: http://localhost:5001

### Comandos Ãºteis

```bash
# Executar em background
docker-compose up -d --build

# Parar os serviÃ§os
docker-compose down

# Ver logs
docker-compose logs -f

# Rebuild apenas um serviÃ§o
docker-compose up --build frontend
docker-compose up --build backend
```

## ğŸ”§ ExecuÃ§Ã£o em Desenvolvimento

### Frontend

1. **Instalar dependÃªncias**
```bash
pnpm install
```

2. **Configurar variÃ¡veis de ambiente**
```bash
cp .env.example .env
# Editar .env conforme necessÃ¡rio
```

3. **Executar em modo desenvolvimento**
```bash
pnpm dev
```

### Backend

1. **Navegar para o diretÃ³rio backend**
```bash
cd backend
```

2. **Instalar dependÃªncias Python**
```bash
pip install -r requirements.txt
```

3. **Configurar classificaÃ§Ã£o IA (opcional)**
```bash
# Editar .env e adicionar sua chave do OpenRouter
OPENROUTER_API_KEY=sk-or-v1-sua-chave-aqui
AI_MODEL=anthropic/claude-3.5-sonnet
AI_CLASSIFICATION_ENABLED=true
```

4. **Executar o servidor Flask**
```bash
python app.py
```

## ğŸ“¡ API Endpoints

### Health Check
```http
GET /api/health
```

### Transformar CSV para SQL
```http
POST /api/transform
Content-Type: application/json

{
  "csvContent": "nome,idade,salario\nJoÃ£o,30,5000.50",
  "fields": [
    {"name": "nome", "selected": true, "order": 0, "format": "text"},
    {"name": "idade", "selected": true, "order": 1, "format": "number"},
    {"name": "salario", "selected": true, "order": 2, "format": "currency"}
  ],
  "tableName": "funcionarios",
  "delimiter": ",",
  "databaseType": "postgresql",
  "includeCreateTable": true
}
```

## ğŸ—‚ï¸ Estrutura do Projeto

```
etl-processor/
â”œâ”€â”€ app/                    # PÃ¡ginas Next.js
â”œâ”€â”€ components/             # Componentes React
â”‚   â”œâ”€â”€ csv/               # Componentes especÃ­ficos para CSV
â”‚   â”œâ”€â”€ dw/                # Componentes de Data Warehouse â­
â”‚   â””â”€â”€ ui/                # Componentes de UI reutilizÃ¡veis
â”œâ”€â”€ lib/                   # UtilitÃ¡rios e serviÃ§os
â”œâ”€â”€ backend/               # API Flask
â”‚   â”œâ”€â”€ app.py            # AplicaÃ§Ã£o principal
â”‚   â”œâ”€â”€ sql_analyzer.py   # Analisador de SQL â­
â”‚   â”œâ”€â”€ dimensional_modeling.py  # Engine de modelagem â­
â”‚   â”œâ”€â”€ star_schema_generator.py # Gerador de Star Schema â­
â”‚   â”œâ”€â”€ requirements.txt  # DependÃªncias Python
â”‚   â””â”€â”€ Dockerfile        # Container do backend
â”œâ”€â”€ docker-compose.yml     # OrquestraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ Dockerfile.frontend    # Container do frontend
â””â”€â”€ README.md             # Este arquivo
```

## ğŸ—ï¸ Modelagem de Data Warehouse

### Funcionalidades AvanÃ§adas

O ETL Processor agora inclui um engine completo de modelagem dimensional que automaticamente:

- **Analisa esquemas SQL** e identifica padrÃµes dimensionais
- **Gera modelos Star Schema** com tabelas fato e dimensÃ£o
- **Cria chaves substitutas** (surrogate keys) automaticamente
- **Implementa SCD** (Slowly Changing Dimensions) Tipo 1 e 2
- **Otimiza para diferentes SGBDs** (MySQL, PostgreSQL, SQL Server, etc.)

### Como Usar a Modelagem DW

1. **Gere o SQL** normalmente atravÃ©s do processamento CSV
2. **Clique em "Modelagem DW"** apÃ³s a geraÃ§Ã£o do SQL
3. **Visualize o modelo** dimensional proposto
4. **Baixe os scripts DDL** otimizados para seu SGBD
5. **Implemente no seu Data Warehouse**

### Endpoints da API DW

```bash
# Analisar SQL
POST /api/analyze-sql
{
  "sql": "CREATE TABLE vendas (...)"
}

# Gerar modelo dimensional
POST /api/generate-dw-model
{
  "sql": "CREATE TABLE vendas (...)",
  "dialect": "postgresql"
}

# Obter recomendaÃ§Ãµes
GET /api/dw-recommendations

# Metadados do sistema
GET /api/dw-metadata
```

## ğŸ¯ Como Usar

1. **Upload do CSV**: Arraste e solte ou selecione um arquivo CSV
2. **Configurar Delimitador**: Escolha o delimitador usado no arquivo
3. **Selecionar Campos**: Escolha quais campos incluir e defina a ordem
4. **Formatar Dados**: Configure o formato de cada campo (texto, nÃºmero, moeda, data)
5. **PrÃ©-visualizar**: Veja como ficarÃ¡ o resultado final
6. **Exportar**: Baixe o arquivo processado (CSV ou SQL)
7. **Modelagem DW** â­: Gere automaticamente um modelo dimensional

## ğŸ”§ ConfiguraÃ§Ãµes Suportadas

### Delimitadores
- VÃ­rgula (`,`)
- Ponto e vÃ­rgula (`;`)
- TabulaÃ§Ã£o (`\t`)
- Pipe (`|`)

### Formatos de Campo
- **Texto**: MantÃ©m como string
- **NÃºmero**: Formata como nÃºmero decimal
- **Moeda**: Formata como valor monetÃ¡rio (R$)
- **Data**: Converte YYYY-MM-DD para DD/MM/YYYY

### Banco de Dados Suportado
- **PostgreSQL** (Ãºnico banco suportado para garantir compatibilidade total)

### Modelos de IA Suportados (OpenRouter)
- `anthropic/claude-3.5-sonnet` (recomendado)
- `anthropic/claude-3-haiku`
- `openai/gpt-4o`
- `openai/gpt-3.5-turbo`
- `meta-llama/llama-3.1-8b-instruct`

## ğŸ¤– ClassificaÃ§Ã£o IA de DimensÃµes

O sistema inclui classificaÃ§Ã£o inteligente de dimensÃµes usando IA via OpenRouter:

### ConfiguraÃ§Ã£o
1. Obtenha uma chave API em [OpenRouter](https://openrouter.ai/)
2. Configure no arquivo `.env`:
```bash
OPENROUTER_API_KEY=sk-or-v1-sua-chave-aqui
AI_MODEL=anthropic/claude-3.5-sonnet
AI_CLASSIFICATION_ENABLED=true
```

### Funcionalidades
- **ClassificaÃ§Ã£o AutomÃ¡tica**: IA analisa colunas e identifica padrÃµes dimensionais
- **Fallback Inteligente**: Se a IA falhar, usa classificaÃ§Ã£o baseada em regras
- **MÃºltiplos Modelos**: Suporte a diferentes modelos de IA
- **ConfianÃ§a**: Cada classificaÃ§Ã£o inclui nÃ­vel de confianÃ§a
- **RaciocÃ­nio**: IA explica suas decisÃµes de classificaÃ§Ã£o

### Endpoint de Teste
```http
POST /api/test-ai-classification
Content-Type: application/json

{
  "sql": "CREATE TABLE dados (nome VARCHAR(255), cpf VARCHAR(255));"
}
```

## ğŸ› Troubleshooting

### Problemas Comuns

1. **Porta 5001 em uso**
   - No macOS, desabilite o AirPlay Receiver em ConfiguraÃ§Ãµes do Sistema
   - Ou altere a porta no arquivo `backend/app.py` e `docker-compose.yml`

2. **Erro de CORS**
   - Verifique se o backend estÃ¡ rodando na porta correta
   - Confirme a variÃ¡vel `NEXT_PUBLIC_API_URL` no `.env`

3. **Erro de parsing CSV**
   - Verifique se o delimitador estÃ¡ correto
   - Certifique-se de que o arquivo estÃ¡ em UTF-8

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ”§ Troubleshooting

### Problema: "Could not generate a valid star schema from the provided SQL"

**SoluÃ§Ã£o**: Este erro foi corrigido na versÃ£o atual. O sistema agora:
- âœ… Suporta tabelas Ãºnicas geradas a partir de CSV
- âœ… Identifica automaticamente padrÃµes dimensionais em dados planos
- âœ… Cria modelos Star Schema mesmo com dados de controle de acesso
- âœ… Ignora comentÃ¡rios SQL durante o parsing

### Outros Problemas Comuns

- **Porta 5001 em uso**: Execute `lsof -ti:5001 | xargs kill -9` para liberar a porta
- **Docker nÃ£o inicia**: Verifique se o Docker Desktop estÃ¡ rodando
- **Erro de CORS**: Verifique se o backend estÃ¡ rodando na porta correta

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸš€ PrÃ³ximas Funcionalidades

- [ ] Suporte a mais formatos de arquivo (Excel, JSON)
- [ ] ValidaÃ§Ã£o avanÃ§ada de dados
- [ ] Templates de transformaÃ§Ã£o salvos
- [ ] API de batch processing
- [ ] Interface de administraÃ§Ã£o
- [ ] Logs de auditoria
